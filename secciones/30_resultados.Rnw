\chapter{Resultados}

<<>>=
tipo_bat <- c(PLOMO = 1L, LITIO = 0L)
@

<<>>=
# preparación de tabla "atributos"
tanque_baterias <- atr_raw |>
 pivot_wider(names_from = "name", values_from = "value") |>
 clean_names() |>
 rename(mtto = periodicidad_mg,
        tanque = capacidad_tanque_motor) |>
 filter(motor_generador == "SI") |>
 select(-motor_generador, -tipo_motor) |>
 mutate(across(tipo_bateria, recode, !!!tipo_bat),
        across(mtto:tipo_bateria, as.factor),
        tanque = case_when(
         siteid == "GUA030" & is.na(tanque) ~ "155",
         siteid == "GUA073" & is.na(tanque) ~ "157",
         siteid == "GUA958" & tanque == "OTRO" ~ "157",
         TRUE ~ tanque),
         across(tanque, parse_integer))
@

<<>>=
# limpieza y feature engineering
mg_clean <- raw_mg |>
 mutate(
  status = if_else(status >= 1, "down", "up"),
  # status = factor(status, levels = c("up","down")),  # original
  status = factor(status, levels = c("down","up")),
  across(c("duracion", "working"), ~ .x * 60),
  across(inicio:fin, ymd_hms),
  across(arranco, as.factor),
  dia = as.factor(day(inicio)),
  mes = as.factor(month(inicio)),
  .after = "fin")
@

<<>>=
# la fecha de inicio y de fin sirven para poder extraer el galonaje que tenía el MG al
# momento de la interrupción del servicio.
# mg_set <- mg_clean |>
#  select(siteid,
#         fecha_inicio = inicio,
#         fecha_fin    = fin)
@

<<eval=FALSE>>=
# debido a que el proceso de extraer los datos de la BD es muy tardado, este chunk no
# lo corremos, sino solo una vez para luego guardar los datos en un formato binario.
# galonaje2 <- pmap_dfr(mg_set, ~ extraer_retina(conx = con,
#                                               siteid = ..1,
#                                               fecha_inicio = ..2,
#                                               fecha_fin = ..3) |>
#       collect()) |>
#  mutate(across(contains("powercut"), ymd_hms))
@

<<>>=
# galonaje <- read_fst("./galonaje.fst") |> as_tibble() |>
#  rename(inicio = inicio_powercut, fin = fin_powercut)
@

<<>>=
# mg <-
mg <- mg_clean |>
 left_join(tanque_baterias, by = "siteid")
@


\section{Análisis Exploratorio}

\subsection{Estructura}

<<>>=
mg %>% glimpse()
@

El tamaño del dataset es de 1902 observaciones con 11 features y una variable respuesta.

\begin{figure}[H]
<<fig.width=7>>=
plot_intro(mg, ggtheme = yunkel, title = "Resumen")
@
\caption{Información básica del dataset}
   \label{fig:info}
\end{figure}

Se ve que no hay valores atípicos y que la mayoría de las features son discretas.

\subsection{Variable dependiente}

\begin{figure}[H]
<<>>=
mg |>
 barra(status) +
 labs(title = "Clasificación desequilibrada")
@

\caption{El desafío de trabajar con conjuntos de datos desequilibrados es que la mayoría de
las técnicas de aprendizaje automático ignorarán y, a su vez, tendrán un rendimiento
deficiente en la clase minoritaria, aunque normalmente lo más importante es el rendimiento en
la clase minoritaria.}
   \label{fig:desbalance}
\end{figure}

En el gráfico \ref{fig:desbalance} se observa que la variable no está equilibrada. La clase
\va{up} tiene más casos que la clase \va{down}. \textbf{Si no balanceamos los datos entonces
lo que pasará es que nuestro modelo aprenderá de manera muy eficaz sobre cómo predecir el
caso negativo, es decir, cuando un sitio no se cae}

\subsection{Medidas repetidas}

<<microcortes>>=
mg %>%
 mutate(ano = year(inicio)) |>
 janitor::get_dupes(siteid, dia, mes, ano) |>
 filter(siteid == "GUA025", dia == 25, mes == 8, ano == 2020) |>
 select(siteid, inicio, fin, status, duracion, working) |>
 tabla(cap = "Ejemplo de microcortes en el servicio de energía eléctrica")
@

En la tabla \ref{tab:microcortes} se aprecia que hay casos en los que en un sitio, en un
mismo día, se pueden presentar hasta 9 apagones.  La mayoría de poca duración, pero incluso
es posible que el sitio pase sin energía hasta 33 días.

\subsection{Atípicos}

<<>>=
var_ok <- mg |>
 nearZeroVar(saveMetrics = T) |>
 rownames_to_column("var") %>%
 filter(!if_any(zeroVar:nzv, ~ .x == TRUE)) |>
 pull(var)
@

Debido a que las variables están en diferentes magnitudes y lo que nos interesa es evaluar la
presencia de atípicos, realizaremos una transformación a los valores utilizando la
\textbf{transformación del logaritmo ajustado}: $log(Y + 1)$. De esta forma los valores con 1
se convertirán en cero al aplicar el logaritmo.

<<>>=
sl <- mg %>%
 select(all_of(var_ok)) %>%
 select(status, duracion, working, tanque) |>
 mutate(across(where(is.numeric), ~ log(.x + 1)))
@

Para graficar no será suficiente la transformación logarítmica, así que aplicaremos una
segunda transformación a nivel del eje $Y$. Esto nos permitirá ver de forma ordenada las
variables con información que tienen atípicos separada por cada uno de los distintos tipos de
status.

\begin{figure}[H]
<<fig.width=7>>=
sl %>%
 pivot_longer(cols = where(is.numeric),
              names_to = "variable",
              values_to = "valor") |>
 ggplot(aes(reorder_within(variable, valor, status, fun = median), valor)) +
 geom_boxplot(outlier.color = "red") +
 scale_x_reordered(name = "feature") +
 # scale_y_log10() +
 facet_grid(status ~ ., scales = "free") +
 drako +
 theme(axis.text.x  = element_text(angle = 90, vjust = 0.5, hjust = -0.01),
       axis.title.x = element_blank()) +
 labs(title        = "Atípicos con variables informativas",
      subtitle     = "Escala logarítmica")
@

\caption{Análisis de valores atípicos posterior a eliminar columnas con varianza cercana a cero}
   \label{fig:atip}
\end{figure}

Observamos en la figura \ref{fig:atip} que hay atípicos. Aunque el tratamiento de atípicos
puede mejorarse a través de una técnica llamada \emph{spatial sign} \citep[pag. ~71]{kuhn_applied_2013}
es posible que se deban investigar en profundidad la razón de estos outliers.

\subsection{Variables numéricas}

<<>>=
mg_n <- mg %>% select(where(is.numeric), -tanque)
@

\begin{figure}[H]
<<fig.width=6>>=
mg_n |>
 pivot_longer(cols = everything()) |>
 ggplot(aes(x = value, y = ..density..)) +
 geom_histogram(fill = "#56B4E9", size = .2, color = "white") +
 geom_density(size = 1) +
 scale_x_log10(name = "Duración en minutos") +
 facet_grid(. ~ name, scales = "free") +
 geom_vline(aes(xintercept = mean(value)), color = "red", linetype = "dashed") +
 geom_vline(aes(xintercept = median(value)), color = "darkgreen", linetype = "dashed") +
 labs(title = "Distribución Duración y Working") +
 drako
@
\caption{Análisis de la distribución de las variables continuas}
   \label{fig:cont}
\end{figure}


\textbf{\hl{La presencia de valores atípicos será determinante en la selección de los
modelos a utilizar.}}


Se ve en la gráfica \ref{fig:cont} que ambos features tienen un sesgo muy pronunciado hacia
la derecha. Lo que esperaríamos ver es una mayor coincidencia entre la duración de la
interrupción de la energía eléctrica y el tiempo que el motor estuvo trabajando, sin embargo,
vemos casos en que el motor estuvo trabajando más de 1000 minutos (16 horas) seguidas.

\begin{figure}[H]
<<fig.width=7>>=
mg_n %>%
 mutate(across(everything(), ~ log(.x + 2))) |>
 ggpairs(data = _, lower = list(continuous = loess_lm),
             upper = list(continuous = wrap("cor", size = 5))) + drako
@
\caption{Interpretación de resultados}
   \label{fig:biv}
\end{figure}

En la figura \ref{fig:biv}, a como se esperaba la relación entre el tiempo que duró la
interrupción y el tiempo que estuvo trabajando el motor es lineal en la mayoría de los casos.
La línea diagonal principal sugiere que hay muchos casos en donde la duración de la
interrupción coincide con la duración del tiempo que trabajó el motor.

\subsection{Variables categóricas}

Ahora se verá la distribución de las variables categóricas, primero a nivel de
general y luego en función de la variable dependiente.

<<>>=
sbm <- mg |>
 select(where(is.factor))
@


<<>>=
meses <- month.name %>% enframe() %>% deframe()
@

\begin{figure}[H]
<<fig.width=7>>=
sbm |>
 mutate(across(mes, recode, !!!meses)) |>
 ggplot(aes(y = fct_rev(fct_infreq(mes)))) +
 geom_bar(aes(fill = arranco)) +
 ylab("Mes") +
 facet_grid(cols = vars(status), scale = "free") +
 scale_fill_OkabeIto() +
 drako
@
\caption{Caída de sitios por mes considerando si el MG funcionó o no}
   \label{fig:caidas}
\end{figure}

Se ve en la gráfica \ref{fig:caidas} que la mayoría de casos en los que hubo caída de
servicios fue en los meses de febrero, mayo y junio. No se puede decir que la mayoría de
los eventos de caída en febrero se debieron principalmente a que el MG no entró a funcionar.
Lo mismo para el resto de meses.

\section{Modelado}

Para una mejor comprensión, en esta sección se agregará código con el fin de explicar de
forma más clara los procedimientos realizados.

\subsection{Split}

<<echo=TRUE>>=
mgt <- mg |> select(-c(siteid:fin))
@

<<>>=
mgt |>
 slice_sample(n = 5) |>
 tabla("Set de datos")
@

<<echo=TRUE>>=
set.seed(2022)
mg_split <- initial_split(data = mgt, strata = status, prop = 0.8)
train    <- training(mg_split)
test     <- testing(mg_split)
@

Posterior a la partición inicial, se realizará una partición sobre los datos de entrenamiento
para que se cuente con un conjunto de validación que ayude a realizar todas las iteraciones
necesarias descritas en la figura \ref{fig:modelpr}, entre ellas, agregar o quitar features,
modificar hiperparámetros, probar diferentes modelos y cambiar pasos de preprocesamiento.

<<echo=TRUE>>=
mg_valid <- validation_split(train, prop = .8, strata = status)
@

<<echo=TRUE>>=
particion <- validation_split(train, prop = 0.7, strata = status)
mg_train <- particion$splits[[1]] |> analysis()
mg_valid <- particion$splits[[1]] |> assessment()
@

<<>>=
split_df <- tibble(
  dataset = c("dataset_original", "train", "test", "mg_train", "mg_valid"),
  size  = c(nrow(mg), nrow(train), nrow(test), nrow(mg_train), nrow(mg_valid)))
@

<<spending>>=
split_df |>
 tabla(cap = "Data Spending")
@

En la tabla \ref{tab:spending} se observa la forma en que se han distribuido los datos
a partir del dataset original.

\subsection{Cross-Validación}

<<echo=TRUE>>=
(mg_folds <- vfold_cv(mg_train, v = 10, strata = status))
@

\subsection{Recetas}

Se crearán varias recetas para poder probar con los distintos métodos que ayudan a corregir
el desequilibrio entre las distintas clases.

<<echo=TRUE>>=
# contiene todos las transformaciones convenientes para el problema
receta_all <- recipe(formula = status ~ ., data = mg_train) %>%
  step_novel(dia, mes, mtto) |>
  step_other(dia, mes, threshold = 0.01) |>
  step_nzv(all_predictors()) |>
  step_normalize(all_numeric()) |>
  step_corr(all_numeric_predictors()) |>
  step_spatialsign(all_numeric_predictors()) |>
  step_dummy(all_nominal(), -all_outcomes()) |>
  step_rose(status, skip = TRUE)
@

A continuación veamos como se vería el conjunto de entrenamiento una vez que se han aplicado
todas las transformaciones especificadas en esta receta.

<<>>=
receta_all |> prep() |> juice() |> glimpse()
@

Ahora veamos si el algoritmo ROSE balancea correctamente las clases:

\begin{figure}[H]
<<>>=
receta_all |>
 prep() |>
 juice() |>
 barra(status) + labs("Clasificación Balanceada")
@

\caption{Variable respuesta balanceada}
   \label{fig:balance}
\end{figure}

Se observa que el procedimiento agregó un mejor balance a la respuesta.

<<echo=TRUE>>=
# las dimensionales de duracion y working están ambas en minutos, no es necesario normalizar
receta_sinnor <- recipe(formula = status ~ ., data = mg_train) %>%
  step_novel(dia, mes, mtto) |>
  step_other(dia, mes, threshold = 0.01) |>
  step_nzv(all_predictors()) |>
  step_corr(all_numeric_predictors()) |>
  step_dummy(all_nominal(), -all_outcomes()) |>
  step_rose(status, skip = TRUE)
@

<<echo=TRUE>>=
# random forest no requiere que se dumifiquen las variables
receta_clean <- recipe(formula = status ~ ., data = mg_train) %>%
   step_novel(dia, mes, mtto) |>
   step_other(dia, mes, threshold = 0.01) |>
   step_nzv(all_predictors()) |>
   step_corr(all_numeric_predictors()) |>
   step_rose(status, skip = TRUE)
@

\subsection{Motores}

<<echo=TRUE>>=
# random_forest
rf_rec <- rand_forest(mtry  = tune(),
                      min_n = tune(),
                      trees = 1000) %>%
    set_engine("ranger", oob.error = TRUE,
               importance = "impurity") %>%
    set_mode("classification")
@

<<echo=TRUE>>=
# regresión_logística
glm_lre <- logistic_reg() |>
 set_engine("glm") |>
 set_mode("classification")
@


<<echo=TRUE>>=
# bagged_tree - aka (bootstrap aggregating)
bag_spec <- bag_tree(min_n = 10) %>%
  set_engine("rpart", times = 25) %>%
  set_mode("classification")
@

\subsection{Workflow}

Agregaremos las recetas y los modelos a una lista nombrada para luego realizar
el tuning de hiperparámetros en una cuadrícula de 20 valores.

<<echo=TRUE>>=
recetas <- list(rose_all = receta_all,
                rose_snr = receta_sinnor,
                rose_cle = receta_clean)
@

<<echo=TRUE>>=
modelos <- list(randon_forest = rf_rec,
                bagging       = bag_spec,
                regresion_logistica = glm_lre)
@

\subsection{Flujo}

Se creará una una combinación de todas las recetas con todos los modelos al establecer
el parámetro \ti{cross = TRUE}.

Esto solo es posible debido a que los modelos requieren prácticamente los mismos pasos de
preprocesamiento, la excepción es random forest, el cual no requiere prácticamente ningún
tipo de preprocesamiento (transformación) previa, sin embargo, para esto creamos una
receta \emph{limpia} para evaluar el comportamiento.

<<echo=TRUE>>=
mg_workflow <- workflow_set(preproc = recetas,
                            models  = modelos,
                            cross   = TRUE)
@

<<>>=
mg_workflow
@

La columna \ti{option} se utiliza en caso de que hubiésemos agregado opciones a algunos de
los modelos. La columna ti{result} indica si se ha optimizado los parámetros de los modelos.
En este caso, en la definición de las motores hemos aplicado las función \ti{tune()} para
que se ajusten los hiperparámetros.


\subsection{Cuadrícula}

<<echo=TRUE>>=
race_ctrl <- control_race(
      save_pred     = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE)
@

\subsection{Métricas}

Antes definamos el set de métricas para evaluar el modelo.

<<echo=TRUE>>=
mset <- metric_set(precision, recall, mn_log_loss, roc_auc, specificity)
@

<<>>=
mset |>
 as_tibble() |>
 tabla("Métricas de evaluación definidas")
@

\subsection{Tuning}

A continuación realizamos el ajuste de modelos y recetas, definidas en el objeto
mg\_workflow, utilizando los remuestreos, el conjunto de métricas y una
cuadrícula de tamaño 20.

<<eval=FALSE, echo=TRUE>>=
all_cores <- detectCores(logical = FALSE)
clusterpr <- makePSOCKcluster(all_cores)
registerDoParallel(clusterpr)
set.seed(2022)
@

<<>>=
# corrida manual
# ruta <- fs::path_wd("modelos")
# tablero <- board_folder(path = ruta)

#corrida en knit
tablero <- board_folder(path = "../modelos")
@

<<eval=FALSE, echo=TRUE>>=
# 5.36 min
tic()
tune_res <- mg_workflow %>%
  workflow_map(
    "tune_race_anova",
    resamples = mg_folds,
    grid = 20,
    control = race_ctrl,
    metrics = mset,
    seed = 2022,
    verbose = TRUE)
toc()
@


<<eval=FALSE>>=
stopCluster(clusterpr)
unregister()
@

<<eval=FALSE>>=
pin_write(board = tablero,
			 x     = tune_res,
			 name  = "tune_res",
			 type  = "rds",
			 title = "modelos_arboles",
			 description = "nuevos_modelos")
@

<<>>=
tune_res <- pin_read(board = tablero, name = "tune_res")
@


<<echo=TRUE>>=
tune_res
@

Vemos que las columnas \ti{option} y \ti{result} se han actualizado. La columna \ti{result}
en particular ahora muestra que los parámetros de los modelos fueron tuneados. El símbolo [+]
indica que no hay ningún problema presente con los objetos creados.

Veamos cuantos modelos fueron ajustados:

<<echo=TRUE>>=
(tm <- nrow(collect_metrics(tune_res, summarize = FALSE)))
@

Ahora veamos los resultados de las métricas con nuestro conjunto de entrenamiento.

<<echo=TRUE>>=
tune_rank <- tune_res %>%
	rank_results(select_best = TRUE, rank_metric = "recall") %>%
	select(modelo = wflow_id, .metric, mean, rank) %>%
	pivot_wider(names_from = .metric, values_from = mean) |>
 relocate(modelo, rank, recall, precision)
@

<<>>=
tune_rank |>
 tabla(cap = "Ranking de los mejores ajustes con datos de entrenamiento")
@

En la figura vemos las combinaciones de los mejores modelos con diferentes motores y
algoritmos de balanceo. La especificidad es la proporción de ceros estimados como ceros

Seleccionemos los mejores modelos de cada tipo, considerando el recall como
el principal.

<<echo=TRUE>>=
best <- tune_rank %>%
 slice(2:9) |>
	pull(modelo) %>%
	set_names(.)
@

<<echo=TRUE>>=
metricas_train <- tune_rank %>%
	filter(modelo %in% best)
@

<<>>=
metricas_train |>
 tabla("Metricas para el conjunto de entrenamiento")
@


<<fig.width=8>>=
autoplot(tune_res, select_best = TRUE) + drako
@

<<echo=TRUE>>=
lista_mejores <- best %>%
	map(~ tune_res %>% extract_workflow_set_result(id = .x) %>%
					       select_best(metric = "recall"))
@

\subsection{Validación}

<<>>=
validation_result_list <- pin_read(board = tablero, name = "validation")
@

<<echo=TRUE, eval=FALSE>>=
validation_result_list <- map2(.x = best, .y = lista_mejores, ~ tune_res %>%
	extract_workflow(id = .x) %>%
	finalize_workflow(.y) %>%
	last_fit(split = particion$splits[[1]], metrics = mset))
@

<<eval=FALSE>>=
pin_write(board = tablero,
			 x     = validation_result_list,
			 name  = "validation",
			 type  = "rds",
			 title = "resultados_validacion",
			 description = "conjunto_validacion")
@


<<echo=TRUE>>=
metricas_validation <- validation_result_list %>%
	map_dfr(~ collect_metrics(.x), .id = "modelo") %>%
	pivot_wider(names_from = .metric, values_from = .estimate) %>%
	select(-c(.estimator:.config))
@

<<echo=TRUE, meval>>=
metricas_validation |> tabla("Métricas de validación")
@

Veamos nuestro set de validación y verifiquemos cuales son los verdaderos valores de la
variable respuesta:

<<>>=
particion$splits[[1]] |>
 assessment() |>
 count(status) |>
 tabla("Tabla de verdad para datos de validación")
@

Veamos la matriz de confusión para los datos de validación para todos los modelos:

<<echo=TRUE>>=
mx <- validation_result_list |>
 map_dfr(~ collect_predictions(.x), .id = "modelo") |>
 group_by(modelo) |>
 conf_mat(status, .pred_class) |>
 rename(mc = conf_mat) |>
 mutate(mc = set_names(mc, best))
@


<<echo=TRUE>>=
mtx <- mx |>
 mutate(graficas = map2(.x = mx$mc, .y = best, ~ autoplot(.x, type = "heatmap") +
                                                 ggtitle(label = .y)))
@

\begin{figure}[H]
<<echo=TRUE, fig.width=8, fig.height=18>>=
mtx$graficas |>
 reduce(.f = `+`) +
 plot_layout(ncol = 2) +
 plot_annotation(title = "Matriz de Confusión con el conjunto de validación")
@
\caption{Interpretación de resultados}
   \label{fig:matcon}
\end{figure}


\begin{snugshade}
Con base a los valores \textbf{verdaderos} observados en el conjunto de validación, se ve que
hay 28 casos down (se cayó el sitio) y 429 casos up (no se cayó el sitio).  Observamos en la
figura \ref{fig:matcon} que el mejor modelo en términos de \emph{sensitivity} es
\ti{rose\_all\_bagging} debido a que de 28 casos verdaderos, logró predecir el 100\%.
\end{snugshade}

\subsection{Precision-recall tradeoff}

\begin{figure}[H]
<<>>=
validation_result_list |>
 pluck("rose_all_bagging") |>
 collect_predictions() |>
 pr_curve(status, .pred_down) |>
 ggplot(aes(x = recall, y = precision)) +
 geom_path() +
 geom_point() +
 coord_equal() +
 labs(title = "Precision and Recall Trade-Off")
@
\caption{Trade-Off}
   \label{fig:prere}
\end{figure}


Se ve en la gráfica  \ref{fig:prere} que el resultado de la tabla \ref{tab:meval} en el que
vemos que al menos a nivel de validación el recall fue perfecto, pero a expensas de una precisión
muy baja.  Esta precisión es baja debido a que la especificidad nos dice que le costó
trabajo identificar los casos en que estaba up.

\subsection{Test}

Una vez que tenemos el modelo que consideramos que lo hace un poco mejor, lo vamos a
evaluar contra los datos de prueba.

<<echo=TRUE>>=
mejorcito <- metricas_validation |>
 slice_max(recall) |>
 pull(modelo)
@


<<echo=TRUE>>=
test_result_list <- map2(.x = best[mejorcito], .y = lista_mejores[mejorcito], ~ tune_res %>%
	extract_workflow(id = .x) %>%
	finalize_workflow(.y) |>
	last_fit(split = mg_split, metrics = mset))
@

<<>>=
metricas_test <- test_result_list %>%
	map_dfr(~ collect_metrics(.x), .id = "modelo") %>%
	pivot_wider(names_from = .metric, values_from = .estimate) %>%
	select(-c(.estimator:.config))
@

<<>>=
metricas_test |> tabla("Métricas con el conjunto de prueba")
@

\subsection{Hiperparámetros}

<<>>=
autoplot(tune_res, id = mejorcito, metric = "recall") + drako
@

\subsection{Importancia de los predictores}

<<echo=TRUE>>=
modelo <- bag_spec |>
 finalize_model(lista_mejores[[mejorcito]]) |>
 fit(status ~ ., data = train)
@


\begin{figure}[H]
<<echo=TRUE>>=
modelo$fit$imp |>
 ggplot(aes(x = value, y = fct_reorder(term, value))) +
 geom_col(fill = "#0072B2", width = 0.6) +
 scale_x_continuous(name = NULL, expand = c(0, 0)) +
	scale_y_discrete(name = NULL, expand = c(0, 0.5)) +
 theme_minimal_vgrid(font_family = "yano") +
	theme(axis.text.y = element_text(size = 14),
				   plot.title = element_text(size = 22, face = "bold"))
@

\caption{Importancia relativa de los predictores}
   \label{fig:varimp}
\end{figure}

\begin{snugshade}
Según la figura \ref{fig:varimp} parece ser que la variable más importante a la hora de
predecir si un sito se caerá por problemas de energía es la duración de la interrupción
de energía, seguido por el tiempo que el motogenerador funciona.
\end{snugshade}

\subsection{Predicción}

<<echo=TRUE>>=
prediccion <- modelo |>
 predict(new_data = test, type = "prob")
@

<<>>=
prediccion |>
 slice_sample(n = 5) |>
 tabla(cap = "Predicción de tipo probabilidad en los datos de prueba")
@


\section{Threshold}

Se establecerá la probabilidad de corte en 0.5. Si la probabilidad de caída del sitio es
mayor a 0.5 se dirá que el sitio se va a caer y si es menor a 0.5 diremos que no se va a
caer (al menos en el corto plazo).

<<echo=TRUE>>=
pred_clase <- prediccion |>
 slice_sample(n = 5) |>
 mutate(predicion_down = if_else(.pred_down > 0.5, 1, 0))
@

<<>>=
pred_clase |>
 slice_sample(n = 20) |>
 tabla(cap = "Vista de la clasificación realizada utilizando el Threshold definido")
@

El siguiente paso es evaluar la precisión del modelo, para eso se puede calcular la
proporción de observaciones que se han clasificado correctamente:

Ajustaremos primero el dataset de prueba para que podamos realizar la comparación posterior
al establecimiento del threshold

<<echo=TRUE>>=
new_test <- test |>
  mutate(status2 = if_else(status == "up", 0, 1))
@

<<echo=TRUE>>=
(ta <- mean(pred_clase$predicion_down == new_test$status2))
@

Se obtuvo una tasa del 95\% de casos de caídas clasificados correctamente.

\section{Ejemplos de falstos positivos y negativos}

<<>>=
pred_clase |>
 bind_cols(new_test |> select(verdadero_status = status2)) |>
 filter(predicion_down == 1, verdadero_status == 0) |>
 tabla(cap = "Ejemplo de falsos positivos")
@

<<>>=
pred_clase |>
 bind_cols(new_test |> select(verdadero_status = status2)) |>
 filter(predicion_down == 0, verdadero_status == 1) |>
 tabla(cap = "Ejemplo de falsos negativos")
@

\section{¿Cómo podría mejorar el modelo?}

